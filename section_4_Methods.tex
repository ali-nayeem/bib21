\section{Methods}
\label{sec:method}

\begin{figure*}[!htbp]%
	\begin{adjustwidth}{-1.1cm}{}
		\centering
		\subfloat[Input-output]{\includegraphics[width=0.28\textwidth]{PMAO}}%
		\subfloat[A high-level workflow for one weight vector]{\label{fig:PMAO:flow}\input{method-flowchart-subfloat}}%
		\subfloat[30 well-spaced weight vectors]{\label{fig:weight}\includegraphics[width=0.35\textwidth]{30-weight.pdf}}
	\end{adjustwidth}
	\caption{A simplified illustration of our PMAO framework.}
	\label{fig:PMAO}
\end{figure*}

\subsection{Application-aware objective functions}
We adopt the following four simple objective functions,identified by~\cite{nayeem2020multiobjective} based on their better correlation to the tree accuracy, to be used in PASTA alongside ML score. Several pairs of the these objectives may have conflicting relationship~\cite{nayeem2020multiobjective}.  
\begin{enumerate}
	\item Maximize similarity for columns containing gaps (SIMG): For each column of the MSA having at least one gap, it calculates the ratio of the most frequent characters. Then all those ratios are added to get the SIMG score.
	\item Maximize similarity for columns containing no gaps (SIMNG): Similar to SIMG except that it considers those columns of the MSA that do not have any gap.
	\item Maximize sum-of-pairs (SOP): For each pair of aligned sequences in the MSA, it takes the sum of substitution score for the two aligned characters across all columns using a substitution matrix. The addition of all pairwise scores gives the SOP score. In this paper, we use the BLOSUM62 matrix for protein sequences.
	\item Minimize number of gaps (GAP): It is the summation of number of gap characters in each aligned sequences. For the sake of uniformity, we convert this score into a minimization criterion.
\end{enumerate}


\subsection{PMAO framework}
\subsubsection{MO principles}
The goal of an MO algorithm is to generate a set of solutions, popularly known as the Pareto front, that represent the best compromise among the objectives due to conflict nature of some pairs of objectives. Among the several classes of MO algorithms (e.g., pareto-based, decomposition-based, indicator-based, etc), decomposition-based strategies are found effective to face the difficulties in handling many (i.e., more than three) objectives~\cite{li2015many}. These algorithms decompose the task of generating several alternative solutions into many single-objective problems with the help of a set of well-distributed weight vectors popularly known as reference directions. Each weight vector aggregate the different objective scores into a single value that eventually lead to one member of the final solution set.

\subsubsection{Simplified workflow}
We develop a decomposition-based MO framework, namely, PASTA with many application-aware objectives (PMAO) (Figure~\ref{fig:PMAO}) by driving the search process of PASTA by a total of five objectives directed by a 5D weight vector. Figure~\ref{fig:PMAO:flow} depicts a high-level flowchart where the steps inspired by MO approach are marked as green. This workflow is executed for all weight vectors to get alternative solutions and can be performed independently in parallel. We see that, PMAO treats a solution better than the other based on the weighted-sum of five objective values instead of using ML score alone. Also note that, PMAO keeps track of whether an improved solution is generated at the previous iteration through a boolean variable \textit{stocDecom}. It impacts the divide-and-conquer strategy within PASTA by enabling the stochastic decomposition which will be discussed soon. PMAO uses the default behavior of PASTA unless mentioned otherwise.

\subsubsection{Weight vectors}
Although working with a higher number of weight vectors increase the chance of getting better solutions in the solution set, we choose to work with 30 weight vectors to reduce the computational burden as well as to demonstrate the synergy between PASTA and an MO approach since 30 is quite a low number to tackle 5 objectives alone by an MO algorithm~\cite{deb2014evolutionary}. We calculate 30 well-spaced points on a 5D unit simplex using the method suggested by~\cite{ref_dirs_energy} as our weight vectors as shown in Figure~\ref{fig:weight}. %The workflow of

\begin{algorithm}[!htbp]%[!htbp]
	\scriptsize
	\textbf{Input:} $tree$: to be bisected; $maxSize$: max. allowable leaves in a tree; $stocDecom$: triggers the stochastic decomposition 
	%\textbf{Output:} $M$: the modified solution\\
	\begin{algorithmic}[1]
		\caption{min-cluster-size-bisect}
		\label{algo:min-bisect}
		\State{$nodeLeaves \gets$  empty dictionary}
		\For{each node $b$ in post-order-traverse($tree$)}	
		\For{each child $c$ of node $b$}
		\State $nodeLeaves[ch] \gets $ \Call{leaf-count}{$ch$}
		\EndFor 
		\If{ \Call{leaf-count}{$b$} $> maxSize$}
		\If{$stocDecom = False$}
		\State $selected \gets$ the node $x$ with the maximum $nodeLeaves[x]$ value
		\Else	
		\State $selected \gets $ fitness proportionate selection where selection probability of a node $x \propto nodeLeaves[x]$ \Comment{stocastic decompostion} \label{algo:min-bisect:stoc}
		\EndIf
		\EndIf
		\State $t1 \gets $ the subtree of $tree$ rooted at $selected$
		\State remove $selected$ from its parent in $tree$
		\EndFor
		\State \textbf{return} $tree, t1$
		\Statex
		
		\Function{leaf-count}{$node$, $tree$}
		\State $ count $ $\gets$ no. of leaves in the subtree of $tree$ rooted at $node$
		\State \textbf{return} $ count $
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsubsection{Stochastic decomposition}\label{subsec:stocastic}
We also enhance PASTA's divide-and-conquer method in the context of MO principles due to its huge impact on the accuracy of the generated (MSA, tree) pair~\cite{liu2012sate}. The heart of this strategy is a decomposition method that divides the leaves (i.e., unaligned sequences) of the guide tree into disjoint subsets. Since version 1.8.0, PASTA has been using \textit{mincluster} decomposition which minimizes the number of resultant subsets given the maximum allowable members in a subset (\textit{maxSize} parameter in Algorithm~\ref{algo:min-bisect}). This default value of this parameter is set to be the half of the total leaves. \textit{mincluster} strategy is implemented by repeatedly calling a method, namely, \textit{min-cluster-size-bisect}, to bisect a given tree. Scanning the nodes of input tree in post-order manner, it removes the subtree with the maximum number of leaves not exceeding the \textit{maxSize}. We embed some randomness in this method to: (i) help PASTA escaping local optima (ii) increase the diversity of the solution set generated by PMAO. Line~\ref{algo:min-bisect:stoc} of Algorithm~\ref{algo:min-bisect} enforce the idea of stochastic decomposition which randomly pick a subtree with the selection probability proportional to the number of leaves under that subtree.

\begin{algorithm}[!htbp]%[!htbp]
	\scriptsize
	\textbf{Input:} $SIMG, SIMNG, SOP, GAP, ML$: scores to be normalized
	%\textbf{Output:} $M$: the modified solution\\
	\begin{algorithmic}[1]
		\caption{rough-normalization}
		\label{algo:normalize}
		\State $GAP \gets 1.0/GAP$ \Comment{convert into a maximiation score}
		\State $ML \gets -1.0/ML$ \Comment{shift the value range to positive zone}
		\State $ obj \gets [SIMG, SIMNG, SOP, GAP, ML]$
		\State $max \gets$ the maximum value in $obj$
		\State $max \gets$ cast $max$ as integer
		\State $d \gets$ the no. of digits in $max$
		\State $ base \gets 10^{d+1}$
		\State add $base$ to all values in $obj$
		\For{$i \gets$ 0 to 4}
		\State $obj[i] \gets $ \Call{softsign}{$obj[i]$}
		\EndFor
		\State \textbf{return} $obj$
		\Statex
		
		\Function{softsign}{$x$} \label{algo:normalize:softsign}
		\State \textbf{return} $ \frac{x}{1 + |x|} $
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\begin{figure}[!htbp]%
	%\centering
	\includegraphics[width=0.4\textwidth]{sigmoid}
	\caption{Some sigmoid functions. Taken from WIKIPEDIA.}
	\label{fig:sigmoid}
\end{figure}

\subsubsection{Normalization}
Finally, we present the way we overcome a hurdle to ensure the desired effect MO strategy within PMAO. Each of the five objective functions are measured using different scales and their ranges differ to a great extent. So without normalization, their weighted-sum may be unexpectedly biased towards the objectives whose range extends far right, on the real number line, than the others. To add further complication, we do not have enough information regarding the distribution (e.g., min, max, avg, etc.) of those objective values. So we design a \textit{rough-normalization} method outlined in Algorithm~\ref{algo:normalize} which is called before the aggregation. Here we first transform the objective values so that each one has the equal number of integer digits by adding an offset. Then we apply the softsign function (line~\ref{algo:normalize:softsign}) on each transformed values to make them close to 1.0. We choose softsign function, over other sigmoid functions (Figure~\ref{fig:sigmoid}), as it converges polynomially (rather than exponentially) which helps to further reduce the risk of the aggregation being unjustly biased towards some objectives. %After applying aforementioned normalization, the weighted-sum of the objective values 


\subsection{Domain-specific performance measure}
As we consider phylogeny estimation as the application domain of MSA, we assess the performance of PMAO by solely based on the ML trees from the its solution set. Therefore, we evaluate the quality of each ML trees with respect to the true phylogenetic tree
using a widely used measure known as the False Negative (FN) rate. FN rate is the percentage
 of edges present in the true tree but missing in the estimated tree. So a small value of FN rate
is desirable. Although there are two more common tree error measures (False Positive (FP rate)
and and Robinson-Foulds (RF) rate), all of them are identical when true and estimated trees are
binary~\cite{warnow2017computational} which is the case in this paper.


\subsection{Datasets}
We conduct experimentation based on BAliBASE 3.0 benchmark~\cite{thompson2005balibase} which is the most widely used alignment databases of protein families. It
provides manually refined reference alignments of high quality based on 3D structural superposition.
It has 218 datasets which are organized into six groups according to their families and similarities: RV11
(very divergent sequences, residue identity below 20\%), RV12 (medium to divergent sequences, 20\%-40\% residue identity), RV20 (families with one or more highly divergent sequences), RV30 (divergent subfamilies), RV40 (sequences with large terminal N/C extensions), and RV50 (sequences
with large internal insertions). To compare among different variants of PASTA and PMAO, we randomly sample 51 datasets and divide them into set A and set B (listed by Table~\ref{tab:balibase}) due to some conveniences.

\begin{table}[!htbp]
	\scriptsize
	%\centering
	\caption{ Datasets selected randomly from BAliBASE 3.0 benchmark.}
	\begin{tabular}{|l|L{3.6cm}|L{3.6cm}|}
		\hline
		\multicolumn{1}{|c|}{Group} & Set A & Set B \\
		\hline
		RV11  & BB11005, BB11018, BB11020, BB11033 & BB11007, BB11019, BB11034, BB11038 \\
		\hline
		RV12  & BB12001, BB12013, BB12022, BB12035, BB12044 & BB12005, BB12026, BB12029,  BB12037\\
		\hline
		RV20  & BB20001, BB20010, BB20022, BB20033, BB20041 & BB20002, BB20012, BB20030, BB20037\\
		\hline
		RV30  & BB30002, BB30008, BB30015, BB30022 & BB30003 BB30011, BB30021, BB30026\\
		\hline
		RV40  & BB40001, BB40013, BB40025, BB40038, BB40048 & BB40006, BB40009, BB40019, BB40033 \\ %
		\hline
		RV50  & BB50001, BB50005, BB50010, BB50016 &  BB50006 BB50002, BB50009, BB50014 \\
		\hline
	\end{tabular}%
	\label{tab:balibase}%
\end{table}%

As we adopt FN rate as our domain-specific performance measure, we generate the true tree for each dataset from the
reference alignments by running RAxML~\cite{stamatakis2014raxml} with bootstrapping, and retaining only the highly
supported edges. 

%\subsection{Machine learning to predict potential weight vectors}


\begin{comment}
\input{method-flowchart}
\begin{figure}%
%\centering
\includegraphics[width=0.35\textwidth]{PMAO}
\caption{Input-output of PMAO framework.}
\label{fig:PMAO2}
\end{figure}
\begin{figure}%
%\centering
\includegraphics[width=0.45\textwidth]{30-weight.pdf}
\caption{30 well-spaced weight vectors.}
\label{fig:weight}
\end{figure}
\end{comment}